{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80189736",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    " \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from collections import defaultdict\n",
    "from textwrap import wrap\n",
    " \n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    " \n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    " \n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    " \n",
    "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
    "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
    " \n",
    "rcParams['figure.figsize'] = 12, 8\n",
    " \n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c894552",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"D:/Codefield/CODE_PYTHON/NLP/Prob1/ABSA/Restaurants_Train_v2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17c3896b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3693, 6)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe2d4d60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3693 entries, 0 to 3692\n",
      "Data columns (total 6 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   id          3693 non-null   int64 \n",
      " 1   content     3693 non-null   object\n",
      " 2   AspectTerm  3693 non-null   object\n",
      " 3   polarity    3693 non-null   object\n",
      " 4   from        3693 non-null   int64 \n",
      " 5   to          3693 non-null   int64 \n",
      "dtypes: int64(3), object(3)\n",
      "memory usage: 173.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "360e70cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_sentiment(polarity):\n",
    "    polarity = str(polarity)\n",
    "    if polarity == 'negative':\n",
    "        return 0\n",
    "    elif polarity == 'neutral':\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "    \n",
    "df['sentiment'] = df.polarity.apply(to_sentiment)\n",
    " \n",
    "class_names = ['negative', 'neutral', 'positive']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d24583d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRE_TRAINED_MODEL_NAME= 'bert-base-cased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20b85c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f27345cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "551dd280",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPReviewDataset(Dataset):\n",
    " \n",
    "    def __init__(self, reviews, targets, tokenizer, max_len):\n",
    "        self.reviews = reviews\n",
    "        #self.review_aspects = review_aspects\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    " \n",
    "    def __getitem__(self, item):\n",
    "        review = str(self.reviews[item])\n",
    "        target = self.targets[item]\n",
    " \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            review,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    " \n",
    "        return {\n",
    "            'review_text': review,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'targets': torch.tensor(target, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09aa5667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3323, 7), (185, 7), (185, 7))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train, df_test = train_test_split(\n",
    "    df,\n",
    "    test_size=0.1,\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "df_val, df_test = train_test_split(\n",
    "    df_test,\n",
    "    test_size=0.5,\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    " \n",
    "df_train.shape, df_val.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "525aa5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "    ds = GPReviewDataset(\n",
    "        reviews=df.content.to_numpy(),\n",
    "        #review_aspects = df.AspectTerm.to_numpy(),\n",
    "        targets=df.sentiment.to_numpy(),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=max_len\n",
    "    )\n",
    " \n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=0\n",
    "    )\n",
    " \n",
    "BATCH_SIZE = 4\n",
    " \n",
    "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb2b921e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "D:\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['review_text', 'input_ids', 'attention_mask', 'targets'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = next(iter(train_data_loader))\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bcf1f0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 160])\n",
      "torch.Size([4, 160])\n",
      "torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "print(data['input_ids'].shape)\n",
    "print(data['attention_mask'].shape)\n",
    "print(data['targets'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebc3f4f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  1109,  1178,  4931,  2546,  1420,  1108,  1103,  2564,  1120,\n",
       "          1103,  2927,   119,   102,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [  101,  1109,  8162,  1110,  1103,  2991, 18311,  7702,  8162,   113,\n",
       "          6199,  1103,  6188,  8162,  1152,  1329,  1107, 13763,   114,   117,\n",
       "          1105,  1103,  4333,  1158,  1110, 15925,  2572,  9012, 14174,  1105,\n",
       "          1304, 16852,  2365,   119,   102,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [  101, 12250,  1106,  1168,  1890,  7724,   117,  1152,  1329,  1103,\n",
       "          4014,  1957,  1106,  9781,  5793,  1120,  1103,  1442,   119,   102,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "        [  101,  1109,  2094,  1110,  1304,  1363,  1111,  1122,   112,   188,\n",
       "          3945,   117,  1618,  1190,  1211, 15688, 17549, 11082,   146,   112,\n",
       "          1396,  1125,   119,   102,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d4c25ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74f2ae9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier(nn.Module):\n",
    " \n",
    "    def __init__(self, n_classes):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME, output_hidden_states=True, output_attentions=True, return_dict=False)\n",
    "        self.drop = nn.Dropout(p=0.3)\n",
    "        self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    " \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        _, pooled_output,_,_ = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        output = self.drop(pooled_output)\n",
    "        return self.out(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a78006d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = SentimentClassifier(len(class_names))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bbb533a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_ids = data['input_ids'].to(device)\n",
    "#attention_mask = data['attention_mask'].to(device)\n",
    " \n",
    "#print(input_ids.shape) # batch size x seq length\n",
    "#print(attention_mask.shape) # batch size x seq length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f006e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.nn.functional.softmax(model(input_ids, attention_mask), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "70983878",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\Python311\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    " \n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    " \n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    " \n",
    "loss_fn = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "47d27ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    model,\n",
    "    data_loader,\n",
    "    loss_fn,\n",
    "    optimizer,\n",
    "    device,\n",
    "    scheduler,\n",
    "    n_examples\n",
    "  ):\n",
    "    model = model.train()\n",
    " \n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    " \n",
    "    for d in data_loader:\n",
    "        input_ids = d[\"input_ids\"].to(device)\n",
    "        #input_ids = input_ids.squeeze(0)\n",
    "        attention_mask = d[\"attention_mask\"].to(device)\n",
    "        #attention_mask = attention_mask.squeeze(0)\n",
    "        #print(input_ids.shape) # batch size x seq length\n",
    "        #print(attention_mask.shape) # batch size x seq length\n",
    "        targets = d[\"targets\"].to(device)\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    " \n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        loss = loss_fn(outputs, targets)\n",
    " \n",
    "        correct_predictions += torch.sum(preds == targets)\n",
    "        losses.append(loss.item())\n",
    " \n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    " \n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "80fd7f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "    model = model.eval()\n",
    " \n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    " \n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            targets = d[\"targets\"].to(device)\n",
    " \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    " \n",
    "            loss = loss_fn(outputs, targets)\n",
    " \n",
    "            correct_predictions += torch.sum(preds == targets)\n",
    "            losses.append(loss.item())\n",
    " \n",
    "    return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2a15680c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "----------\n",
      "Train loss 0.7603523036763161 accuracy 0.7005717724947337\n",
      "Val   loss 0.7255871009993109 accuracy 0.7081081081081081\n",
      "\n",
      "Epoch 2/10\n",
      "----------\n",
      "Train loss 0.5823111337013924 accuracy 0.8161300030093289\n",
      "Val   loss 1.0831179628693597 accuracy 0.7189189189189189\n",
      "\n",
      "Epoch 3/10\n",
      "----------\n",
      "Train loss 0.4987990102826233 accuracy 0.8564550105326513\n",
      "Val   loss 1.3714010282295777 accuracy 0.6864864864864865\n",
      "\n",
      "Epoch 4/10\n",
      "----------\n",
      "Train loss 0.4609108183933052 accuracy 0.8712007222389407\n",
      "Val   loss 1.5176627825130709 accuracy 0.7027027027027027\n",
      "\n",
      "Epoch 5/10\n",
      "----------\n",
      "Train loss 0.42803569395182944 accuracy 0.8775203129702076\n",
      "Val   loss 1.6410929571844617 accuracy 0.7027027027027027\n",
      "\n",
      "Epoch 6/10\n",
      "----------\n",
      "Train loss 0.40811830978738606 accuracy 0.8868492326211255\n",
      "Val   loss 1.6173955467480057 accuracy 0.7135135135135136\n",
      "\n",
      "Epoch 7/10\n",
      "----------\n",
      "Train loss 0.3938438171894451 accuracy 0.8883538970809509\n",
      "Val   loss 1.6096501434327133 accuracy 0.7027027027027027\n",
      "\n",
      "Epoch 8/10\n",
      "----------\n",
      "Train loss 0.3754599330426086 accuracy 0.8895576286488113\n",
      "Val   loss 1.6628367725092215 accuracy 0.7081081081081081\n",
      "\n",
      "Epoch 9/10\n",
      "----------\n",
      "Train loss 0.36817570675358346 accuracy 0.8922660246764972\n",
      "Val   loss 1.6373520443352816 accuracy 0.7081081081081081\n",
      "\n",
      "Epoch 10/10\n",
      "----------\n",
      "Train loss 0.34287885147910796 accuracy 0.8946734878122179\n",
      "Val   loss 1.777834774168212 accuracy 0.7135135135135136\n",
      "\n",
      "CPU times: total: 1d 10h 33min 38s\n",
      "Wall time: 17h 33min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    " \n",
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    " \n",
    "for epoch in range(EPOCHS):\n",
    " \n",
    "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "    print('-' * 10)\n",
    " \n",
    "    train_acc, train_loss = train_epoch(\n",
    "        model,\n",
    "        train_data_loader,\n",
    "        loss_fn,\n",
    "        optimizer,\n",
    "        device,\n",
    "        scheduler,\n",
    "        len(df_train)\n",
    "    )\n",
    " \n",
    "    print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    " \n",
    "    val_acc, val_loss = eval_model(\n",
    "        model,\n",
    "        val_data_loader,\n",
    "        loss_fn,\n",
    "        device,\n",
    "        len(df_val)\n",
    "    )\n",
    " \n",
    "    print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
    "    print()\n",
    " \n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    " \n",
    "    if val_acc > best_accuracy:\n",
    "        torch.save(model.state_dict(), 'best_model_state_2.bin')\n",
    "        best_accuracy = val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2f370c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 160])\n",
      "torch.Size([4, 160])\n"
     ]
    }
   ],
   "source": [
    "##没有什么用的分割线\n",
    "input_ids = data['input_ids'].to(device)\n",
    "attention_mask = data['attention_mask'].to(device)\n",
    " \n",
    "print(input_ids.shape) # batch size x seq length\n",
    "print(attention_mask.shape) # batch size x seq length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "34561674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(history['train_acc.item()'],[1], label='train accuracy')\n",
    "# plt.plot(history['val_acc.item()'],[1], label='validation accuracy')\n",
    "# plt.title('Training history')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.legend()\n",
    "# plt.ylim([0, 1]);\n",
    "# plt.xlim([0,10]);\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f5ac37e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.7006, dtype=torch.float64),\n",
       " tensor(0.8161, dtype=torch.float64),\n",
       " tensor(0.8565, dtype=torch.float64),\n",
       " tensor(0.8712, dtype=torch.float64),\n",
       " tensor(0.8775, dtype=torch.float64),\n",
       " tensor(0.8868, dtype=torch.float64),\n",
       " tensor(0.8884, dtype=torch.float64),\n",
       " tensor(0.8896, dtype=torch.float64),\n",
       " tensor(0.8923, dtype=torch.float64),\n",
       " tensor(0.8947, dtype=torch.float64)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history['train_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c44972c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.7081, dtype=torch.float64),\n",
       " tensor(0.7189, dtype=torch.float64),\n",
       " tensor(0.6865, dtype=torch.float64),\n",
       " tensor(0.7027, dtype=torch.float64),\n",
       " tensor(0.7027, dtype=torch.float64),\n",
       " tensor(0.7135, dtype=torch.float64),\n",
       " tensor(0.7027, dtype=torch.float64),\n",
       " tensor(0.7081, dtype=torch.float64),\n",
       " tensor(0.7081, dtype=torch.float64),\n",
       " tensor(0.7135, dtype=torch.float64)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history['val_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1d09c29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#调用训练好的模型\n",
    "model = SentimentClassifier(len(class_names))\n",
    "model.load_state_dict(torch.load('best_model_state_2.bin'))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "18548b63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7567567567567568"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc, _ = eval_model(\n",
    "    model,\n",
    "    test_data_loader,\n",
    "    loss_fn,\n",
    "    device,\n",
    "    len(df_test)\n",
    ")\n",
    "test_acc.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "09be1c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, data_loader):\n",
    "    model = model.eval()\n",
    " \n",
    "    review_texts = []\n",
    "    predictions = []\n",
    "    prediction_probs = []\n",
    "    real_values = []\n",
    " \n",
    "    with torch.no_grad():\n",
    "        for d in data_loader:\n",
    " \n",
    "            texts = d[\"review_text\"]\n",
    "            input_ids = d[\"input_ids\"].to(device)\n",
    "            attention_mask = d[\"attention_mask\"].to(device)\n",
    "            targets = d[\"targets\"].to(device)\n",
    " \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    " \n",
    "            review_texts.extend(texts)\n",
    "            predictions.extend(preds)\n",
    "            prediction_probs.extend(outputs)\n",
    "            real_values.extend(targets)\n",
    " \n",
    "    predictions = torch.stack(predictions).cpu()\n",
    "    prediction_probs = torch.stack(prediction_probs).cpu()\n",
    "    real_values = torch.stack(real_values).cpu()\n",
    "    return review_texts, predictions, prediction_probs, real_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2ec6652f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\n",
    "    model,\n",
    "    test_data_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "32d66e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No one asked what was wrong as we left with nothing touched on our\n",
      "plates.\n",
      "\n",
      "True sentiment: neutral\n"
     ]
    }
   ],
   "source": [
    "idx = 2\n",
    " \n",
    "review_text = y_review_texts[idx]\n",
    "true_sentiment = y_test[idx]\n",
    "pred_df = pd.DataFrame({\n",
    "    'class_names': class_names,\n",
    "    'values': y_pred_probs[idx]\n",
    "})\n",
    " \n",
    "print(\"\\n\".join(wrap(review_text)))\n",
    "print()\n",
    "print(f'True sentiment: {class_names[true_sentiment]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7ee482ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_text = \"Hate you!!!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ad8856c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_review = tokenizer.encode_plus(\n",
    "    review_text,\n",
    "    max_length=MAX_LEN,\n",
    "    add_special_tokens=True,\n",
    "    return_token_type_ids=False,\n",
    "    pad_to_max_length=True,\n",
    "    return_attention_mask=True,\n",
    "    return_tensors='pt',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cd4a26d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review text: Hate you!!!\n",
      "Sentiment  : positive\n"
     ]
    }
   ],
   "source": [
    "input_ids = encoded_review['input_ids'].to(device)\n",
    "attention_mask = encoded_review['attention_mask'].to(device)\n",
    " \n",
    "output = model(input_ids, attention_mask)\n",
    "_, prediction = torch.max(output, dim=1)\n",
    " \n",
    "print(f'Review text: {review_text}')\n",
    "print(f'Sentiment  : {class_names[prediction]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
